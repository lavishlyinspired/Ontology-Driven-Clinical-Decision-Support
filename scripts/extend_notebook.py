"""
Extend experiment.ipynb with all new capability sections (Phases 1-8)
"""
import json
import uuid

def make_md_cell(source_lines):
    return {
        "cell_type": "markdown",
        "metadata": {},
        "source": source_lines,
        "id": str(uuid.uuid4())[:8]
    }

def make_code_cell(source_lines, outputs=None):
    return {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": outputs or [],
        "source": source_lines,
        "id": str(uuid.uuid4())[:8]
    }

new_cells = []

# ============================================================
# PHASE 1: Conversation / Chat Testing
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 11. Conversational Assistant Testing\n",
    "\n",
    "Interactive testing of the LCA assistant conversation service.\n",
    "Supports patient analysis, follow-up questions, and general Q&A."
]))

new_cells.append(make_code_cell([
    "# Initialize the conversation service for notebook testing\n",
    "import asyncio\n",
    "import json\n",
    "from backend.src.services.lca_service import LungCancerAssistantService\n",
    "from backend.src.services.conversation_service import ConversationService\n",
    "\n",
    "# Initialize with all features enabled\n",
    "lca_service = LungCancerAssistantService(\n",
    "    use_neo4j=True,\n",
    "    use_vector_store=False,\n",
    "    enable_advanced_workflow=True,\n",
    "    enable_provenance=True\n",
    ")\n",
    "\n",
    "conversation = ConversationService(lca_service, enable_enhanced_features=True)\n",
    "print(f'Conversation service initialized. Enhanced features: {conversation.enable_enhanced_features}')\n",
    "print(f'LCA Service ready: {len(lca_service.rule_engine.rules)} rules loaded')"
]))

new_cells.append(make_code_cell([
    "# Helper function to run async chat and display results nicely\n",
    "async def ask_assistant(message: str, session_id: str = 'notebook_session', show_raw=False):\n",
    "    \"\"\"Send a message to the assistant and display the streamed response.\"\"\"\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'USER: {message}')\n",
    "    print(f'{\"=\"*70}\\n')\n",
    "    \n",
    "    full_text = []\n",
    "    async for chunk in conversation.chat_stream(session_id=session_id, message=message):\n",
    "        # Parse SSE format\n",
    "        if chunk.startswith('data: '):\n",
    "            try:\n",
    "                data = json.loads(chunk[6:].strip())\n",
    "                msg_type = data.get('type', '')\n",
    "                content = data.get('content', '')\n",
    "                \n",
    "                if msg_type == 'status':\n",
    "                    print(f'  [{msg_type}] {content}')\n",
    "                elif msg_type == 'reasoning':\n",
    "                    print(f'  {content}')\n",
    "                elif msg_type == 'progress':\n",
    "                    print(f'  > {content}')\n",
    "                elif msg_type == 'text':\n",
    "                    print(content)\n",
    "                    full_text.append(content)\n",
    "                elif msg_type == 'recommendation':\n",
    "                    print(content)\n",
    "                    full_text.append(content)\n",
    "                elif msg_type == 'patient_data':\n",
    "                    if isinstance(content, dict):\n",
    "                        print(f'  Patient Data: {json.dumps(content, indent=2)}')\n",
    "                elif msg_type == 'complexity':\n",
    "                    if isinstance(content, dict):\n",
    "                        print(f'  Complexity: {content.get(\"level\", \"\")} -> {content.get(\"workflow\", \"\")}')\n",
    "                elif msg_type == 'follow_up_suggestions':\n",
    "                    print(f'\\n  Suggested follow-ups:')\n",
    "                    for q in (content if isinstance(content, list) else []):\n",
    "                        print(f'    - {q}')\n",
    "                elif msg_type == 'suggestions':\n",
    "                    print(f'\\n  Suggestions:')\n",
    "                    for s in (content if isinstance(content, list) else []):\n",
    "                        print(f'    - {s}')\n",
    "                elif msg_type == 'error':\n",
    "                    print(f'  ERROR: {content}')\n",
    "                elif msg_type == 'log' and show_raw:\n",
    "                    print(f'  [log] {content}')\n",
    "                elif show_raw:\n",
    "                    print(f'  [{msg_type}] {content}')\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    \n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "print('ask_assistant() helper ready. Usage: await ask_assistant(\"your question\")')"
]))

new_cells.append(make_md_cell([
    "### 11.1 Basic Patient Analysis"
]))

new_cells.append(make_code_cell([
    "# Test 1: Basic patient analysis\n",
    "result = await ask_assistant(\n",
    "    '68-year-old male, stage IIIA adenocarcinoma, EGFR exon 19 deletion positive, PS 1',\n",
    "    session_id='test_basic'\n",
    ")"
]))

new_cells.append(make_code_cell([
    "# Test 2: Follow-up question on the same session\n",
    "result = await ask_assistant(\n",
    "    'Why was this treatment recommended over other options?',\n",
    "    session_id='test_basic'\n",
    ")"
]))

new_cells.append(make_code_cell([
    "# Test 3: Ask for alternatives\n",
    "result = await ask_assistant(\n",
    "    'What alternative treatments are available?',\n",
    "    session_id='test_basic'\n",
    ")"
]))

new_cells.append(make_md_cell([
    "### 11.2 Moderate Complexity Cases"
]))

new_cells.append(make_code_cell([
    "# Test 4: Patient with comorbidities\n",
    "result = await ask_assistant(\n",
    "    '72-year-old female, stage IV squamous cell carcinoma, PD-L1 TPS 80%, PS 2, '\n",
    "    'comorbidities: COPD, cardiovascular disease, diabetes',\n",
    "    session_id='test_moderate'\n",
    ")"
]))

new_cells.append(make_code_cell([
    "# Test 5: Follow-up about comorbidity interactions\n",
    "result = await ask_assistant(\n",
    "    'How do the comorbidities affect the treatment choice? What are the risk factors?',\n",
    "    session_id='test_moderate'\n",
    ")"
]))

new_cells.append(make_md_cell([
    "### 11.3 Complex Cases with Multiple Biomarkers"
]))

new_cells.append(make_code_cell([
    "# Test 6: Complex SCLC case\n",
    "result = await ask_assistant(\n",
    "    '55-year-old male, extensive-stage small cell lung cancer, PS 1, '\n",
    "    'KRAS G12C mutation, PD-L1 TPS 30%',\n",
    "    session_id='test_complex'\n",
    ")"
]))

new_cells.append(make_code_cell([
    "# Test 7: Multi-turn conversation - prognosis\n",
    "result = await ask_assistant(\n",
    "    'What is the expected prognosis and survival outlook for this patient?',\n",
    "    session_id='test_complex'\n",
    ")"
]))

new_cells.append(make_code_cell([
    "# Test 8: Clinical trial eligibility\n",
    "result = await ask_assistant(\n",
    "    'Are there any clinical trials this patient might be eligible for?',\n",
    "    session_id='test_complex'\n",
    ")"
]))

new_cells.append(make_md_cell([
    "### 11.4 General Q&A"
]))

new_cells.append(make_code_cell([
    "# Test 9: General knowledge question\n",
    "result = await ask_assistant(\n",
    "    'What are the latest NICE guidelines for stage IIIA NSCLC treatment?',\n",
    "    session_id='test_qa'\n",
    ")"
]))

new_cells.append(make_code_cell([
    "# Test 10: View conversation history\n",
    "for sid in ['test_basic', 'test_moderate', 'test_complex']:\n",
    "    history = conversation.get_history(sid)\n",
    "    print(f'\\nSession {sid}: {len(history)} messages')\n",
    "    for msg in history:\n",
    "        role = msg['role'].upper()\n",
    "        content = msg['content'][:80] + '...' if len(msg['content']) > 80 else msg['content']\n",
    "        print(f'  [{role}] {content}')"
]))

# ============================================================
# PHASE 2: Neo4j CRUD + Q&A
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 12. Neo4j Data Management & Querying\n",
    "\n",
    "Save, update, and query patient data in Neo4j. Includes Text2Cypher for natural language queries."
]))

new_cells.append(make_code_cell([
    "# Connect to Neo4j\n",
    "import os\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "NEO4J_URI = os.getenv('NEO4J_URI', 'bolt://localhost:7687')\n",
    "NEO4J_USER = os.getenv('NEO4J_USER', 'neo4j')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD', '123456789')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE', 'neo4j')\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "driver.verify_connectivity()\n",
    "print(f'Connected to Neo4j at {NEO4J_URI}')\n",
    "\n",
    "def run_query(query, params=None, database=NEO4J_DATABASE):\n",
    "    \"\"\"Run a Cypher query and return results as list of dicts.\"\"\"\n",
    "    with driver.session(database=database) as session:\n",
    "        result = session.run(query, params or {})\n",
    "        return [dict(record) for record in result]\n",
    "\n",
    "def run_write(query, params=None, database=NEO4J_DATABASE):\n",
    "    \"\"\"Run a write Cypher query.\"\"\"\n",
    "    with driver.session(database=database) as session:\n",
    "        result = session.run(query, params or {})\n",
    "        summary = result.consume()\n",
    "        return summary.counters\n",
    "\n",
    "print('Neo4j helper functions ready.')"
]))

new_cells.append(make_md_cell([
    "### 12.1 Create / Save Patient Data"
]))

new_cells.append(make_code_cell([
    "# Create a test patient in Neo4j\n",
    "create_patient_query = \"\"\"\n",
    "MERGE (p:Patient {patient_id: $patient_id})\n",
    "SET p.age = $age,\n",
    "    p.sex = $sex,\n",
    "    p.tnm_stage = $tnm_stage,\n",
    "    p.histology_type = $histology_type,\n",
    "    p.performance_status = $performance_status,\n",
    "    p.created_at = datetime(),\n",
    "    p.updated_at = datetime()\n",
    "WITH p\n",
    "FOREACH (bm IN $biomarkers |\n",
    "    MERGE (b:Biomarker {marker_type: bm.type, patient_id: $patient_id})\n",
    "    SET b.value = bm.value, b.status = bm.status\n",
    "    MERGE (p)-[:HAS_BIOMARKER]->(b)\n",
    ")\n",
    "FOREACH (cm IN $comorbidities |\n",
    "    MERGE (c:Comorbidity {name: cm})\n",
    "    MERGE (p)-[:HAS_COMORBIDITY]->(c)\n",
    ")\n",
    "RETURN p.patient_id as id, p.age as age, p.tnm_stage as stage\n",
    "\"\"\"\n",
    "\n",
    "test_patients = [\n",
    "    {\n",
    "        'patient_id': 'PAT_NOTEBOOK_001',\n",
    "        'age': 68, 'sex': 'M', 'tnm_stage': 'IIIA',\n",
    "        'histology_type': 'adenocarcinoma', 'performance_status': 1,\n",
    "        'biomarkers': [{'type': 'EGFR', 'value': 'Exon 19 deletion', 'status': 'Positive'}],\n",
    "        'comorbidities': ['COPD']\n",
    "    },\n",
    "    {\n",
    "        'patient_id': 'PAT_NOTEBOOK_002',\n",
    "        'age': 72, 'sex': 'F', 'tnm_stage': 'IV',\n",
    "        'histology_type': 'squamous_cell', 'performance_status': 2,\n",
    "        'biomarkers': [{'type': 'PD-L1', 'value': '80%', 'status': 'High'}],\n",
    "        'comorbidities': ['cardiovascular_disease', 'diabetes']\n",
    "    },\n",
    "    {\n",
    "        'patient_id': 'PAT_NOTEBOOK_003',\n",
    "        'age': 55, 'sex': 'M', 'tnm_stage': 'Extensive',\n",
    "        'histology_type': 'small_cell', 'performance_status': 1,\n",
    "        'biomarkers': [{'type': 'KRAS', 'value': 'G12C', 'status': 'Positive'},\n",
    "                       {'type': 'PD-L1', 'value': '30%', 'status': 'Low'}],\n",
    "        'comorbidities': []\n",
    "    },\n",
    "]\n",
    "\n",
    "for pt in test_patients:\n",
    "    result = run_query(create_patient_query, pt)\n",
    "    print(f'Created/Updated: {result}')\n",
    "\n",
    "print(f'\\nTotal patients created: {len(test_patients)}')"
]))

new_cells.append(make_md_cell([
    "### 12.2 Update Patient Data"
]))

new_cells.append(make_code_cell([
    "# Update a patient's stage and add a treatment decision\n",
    "update_query = \"\"\"\n",
    "MATCH (p:Patient {patient_id: $patient_id})\n",
    "SET p.performance_status = $new_ps,\n",
    "    p.updated_at = datetime()\n",
    "WITH p\n",
    "CREATE (d:TreatmentDecision {\n",
    "    decision_id: randomUUID(),\n",
    "    decision_type: 'treatment_recommendation',\n",
    "    treatment: $treatment,\n",
    "    evidence_level: $evidence,\n",
    "    confidence_score: $confidence,\n",
    "    reasoning: $reasoning,\n",
    "    created_at: datetime()\n",
    "})\n",
    "CREATE (p)-[:RECEIVED_RECOMMENDATION]->(d)\n",
    "RETURN p.patient_id as patient, d.treatment as treatment, d.evidence_level as evidence\n",
    "\"\"\"\n",
    "\n",
    "result = run_query(update_query, {\n",
    "    'patient_id': 'PAT_NOTEBOOK_001',\n",
    "    'new_ps': 1,\n",
    "    'treatment': 'Osimertinib (EGFR TKI)',\n",
    "    'evidence': 'Grade A',\n",
    "    'confidence': 0.92,\n",
    "    'reasoning': 'EGFR Exon 19 del positive, Stage IIIA adenocarcinoma - NICE CG121 R8A'\n",
    "})\n",
    "print(f'Updated patient with treatment: {result}')"
]))

new_cells.append(make_md_cell([
    "### 12.3 Query Patient Data - Basic to Complex"
]))

new_cells.append(make_code_cell([
    "# BASIC: What stage is patient PAT_NOTEBOOK_001?\n",
    "print('=== BASIC QUERY: Patient Stage ===')\n",
    "result = run_query(\"\"\"\n",
    "    MATCH (p:Patient {patient_id: 'PAT_NOTEBOOK_001'})\n",
    "    RETURN p.patient_id as id, p.tnm_stage as stage, p.histology_type as histology, p.age as age\n",
    "\"\"\")\n",
    "for r in result:\n",
    "    print(f'  Patient {r[\"id\"]}: Stage {r[\"stage\"]}, {r[\"histology\"]}, Age {r[\"age\"]}')"
]))

new_cells.append(make_code_cell([
    "# MODERATE: Find all patients with EGFR mutations and their treatments\n",
    "print('=== MODERATE QUERY: EGFR+ Patients & Treatments ===')\n",
    "result = run_query(\"\"\"\n",
    "    MATCH (p:Patient)-[:HAS_BIOMARKER]->(b:Biomarker {marker_type: 'EGFR'})\n",
    "    OPTIONAL MATCH (p)-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "    RETURN p.patient_id as patient, p.tnm_stage as stage,\n",
    "           b.value as egfr_variant, b.status as egfr_status,\n",
    "           d.treatment as treatment, d.evidence_level as evidence\n",
    "\"\"\")\n",
    "for r in result:\n",
    "    print(f'  {r[\"patient\"]}: Stage {r[\"stage\"]}, EGFR {r[\"egfr_variant\"]} -> Treatment: {r.get(\"treatment\", \"None\")}')"
]))

new_cells.append(make_code_cell([
    "# COMPLEX: Treatment outcomes for patients with comorbidities by stage\n",
    "print('=== COMPLEX QUERY: Comorbidity Impact on Treatment Selection ===')\n",
    "result = run_query(\"\"\"\n",
    "    MATCH (p:Patient)-[:HAS_COMORBIDITY]->(c:Comorbidity)\n",
    "    OPTIONAL MATCH (p)-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "    OPTIONAL MATCH (p)-[:HAS_BIOMARKER]->(b:Biomarker)\n",
    "    WITH p, collect(DISTINCT c.name) as comorbidities,\n",
    "         collect(DISTINCT {treatment: d.treatment, evidence: d.evidence_level, confidence: d.confidence_score}) as decisions,\n",
    "         collect(DISTINCT {type: b.marker_type, status: b.status}) as biomarkers\n",
    "    RETURN p.patient_id as patient, p.tnm_stage as stage, p.performance_status as ps,\n",
    "           comorbidities, decisions, biomarkers\n",
    "    ORDER BY p.patient_id\n",
    "\"\"\")\n",
    "for r in result:\n",
    "    print(f'\\n  Patient {r[\"patient\"]} (Stage {r[\"stage\"]}, PS {r[\"ps\"]})')\n",
    "    print(f'    Comorbidities: {\", \".join(r[\"comorbidities\"])}')\n",
    "    print(f'    Biomarkers: {r[\"biomarkers\"]}')\n",
    "    for d in r['decisions']:\n",
    "        if d.get('treatment'):\n",
    "            print(f'    Treatment: {d[\"treatment\"]} (Evidence: {d[\"evidence\"]}, Confidence: {d[\"confidence\"]})')"
]))

new_cells.append(make_md_cell([
    "### 12.4 Text-to-Cypher (Natural Language Queries)\n",
    "\n",
    "Convert natural language questions to Cypher queries using the ontology schema.\n",
    "Adapted from GoingMeta Session 31."
]))

new_cells.append(make_code_cell([
    "# Text2Cypher: Natural language to Cypher query generation\n",
    "from backend.src.ontology.lucada_ontology import LUCADAOntology\n",
    "\n",
    "class Text2Cypher:\n",
    "    \"\"\"Convert natural language questions to Cypher queries using ontology schema.\"\"\"\n",
    "    \n",
    "    def __init__(self, ontology):\n",
    "        self.ontology = ontology\n",
    "        self.schema = self._build_schema()\n",
    "    \n",
    "    def _build_schema(self) -> str:\n",
    "        \"\"\"Build a natural language schema description from the ontology.\"\"\"\n",
    "        onto = self.ontology.create()\n",
    "        lines = ['Neo4j Graph Schema:', '']\n",
    "        lines.append('Node Labels:')\n",
    "        lines.append('  - Patient (patient_id, age, sex, tnm_stage, histology_type, performance_status)')\n",
    "        lines.append('  - Biomarker (marker_type, value, status)')\n",
    "        lines.append('  - Comorbidity (name)')\n",
    "        lines.append('  - TreatmentDecision (decision_id, treatment, evidence_level, confidence_score, reasoning)')\n",
    "        lines.append('  - Guideline (name, source, evidence_level)')\n",
    "        lines.append('  - Diagnosis (type, stage, histology)')\n",
    "        lines.append('')\n",
    "        lines.append('Relationships:')\n",
    "        lines.append('  - (Patient)-[:HAS_BIOMARKER]->(Biomarker)')\n",
    "        lines.append('  - (Patient)-[:HAS_COMORBIDITY]->(Comorbidity)')\n",
    "        lines.append('  - (Patient)-[:RECEIVED_RECOMMENDATION]->(TreatmentDecision)')\n",
    "        lines.append('  - (Patient)-[:HAS_DIAGNOSIS]->(Diagnosis)')\n",
    "        lines.append('  - (TreatmentDecision)-[:APPLIED_GUIDELINE]->(Guideline)')\n",
    "        lines.append('  - (TreatmentDecision)-[:ABOUT]->(Patient)')\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def generate_cypher(self, question: str) -> str:\n",
    "        \"\"\"Generate Cypher from natural language using pattern matching.\"\"\"\n",
    "        q = question.lower()\n",
    "        \n",
    "        # Pattern: 'how many patients'\n",
    "        if 'how many patient' in q:\n",
    "            if 'stage' in q:\n",
    "                return 'MATCH (p:Patient) RETURN p.tnm_stage as stage, count(p) as count ORDER BY count DESC'\n",
    "            if 'biomarker' in q or 'egfr' in q or 'alk' in q:\n",
    "                return 'MATCH (p:Patient)-[:HAS_BIOMARKER]->(b:Biomarker) RETURN b.marker_type as biomarker, count(p) as count ORDER BY count DESC'\n",
    "            return 'MATCH (p:Patient) RETURN count(p) as total_patients'\n",
    "        \n",
    "        # Pattern: 'what treatment' / 'which treatment'\n",
    "        if any(x in q for x in ['what treatment', 'which treatment', 'treatment for']):\n",
    "            if 'egfr' in q:\n",
    "                return '''MATCH (p:Patient)-[:HAS_BIOMARKER]->(b:Biomarker {marker_type: 'EGFR'})\n",
    "                OPTIONAL MATCH (p)-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "                RETURN p.patient_id as patient, b.value as egfr_variant, d.treatment as treatment, d.evidence_level as evidence'''\n",
    "            if 'stage' in q:\n",
    "                import re\n",
    "                stage_match = re.search(r'stage\\s+(\\w+)', q, re.IGNORECASE)\n",
    "                stage = stage_match.group(1).upper() if stage_match else 'IV'\n",
    "                return f\"\"\"MATCH (p:Patient {{tnm_stage: '{stage}'}})-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "                RETURN p.patient_id as patient, p.tnm_stage as stage, d.treatment as treatment, d.evidence_level as evidence\"\"\"\n",
    "            return '''MATCH (p:Patient)-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "            RETURN p.patient_id as patient, d.treatment as treatment, d.evidence_level as evidence ORDER BY d.confidence_score DESC'''\n",
    "        \n",
    "        # Pattern: 'find patients with' / 'show patients with'\n",
    "        if any(x in q for x in ['find patient', 'show patient', 'list patient', 'get patient']):\n",
    "            if 'comorbid' in q:\n",
    "                return '''MATCH (p:Patient)-[:HAS_COMORBIDITY]->(c:Comorbidity)\n",
    "                RETURN p.patient_id as patient, p.tnm_stage as stage, collect(c.name) as comorbidities'''\n",
    "            return 'MATCH (p:Patient) RETURN p.patient_id as id, p.age as age, p.sex as sex, p.tnm_stage as stage, p.histology_type as histology LIMIT 20'\n",
    "        \n",
    "        # Pattern: 'compare' \n",
    "        if 'compare' in q:\n",
    "            return '''MATCH (p:Patient)\n",
    "            OPTIONAL MATCH (p)-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "            OPTIONAL MATCH (p)-[:HAS_BIOMARKER]->(b:Biomarker)\n",
    "            RETURN p.patient_id as patient, p.tnm_stage as stage,\n",
    "                   collect(DISTINCT b.marker_type) as biomarkers,\n",
    "                   collect(DISTINCT d.treatment) as treatments\n",
    "            ORDER BY p.patient_id'''\n",
    "        \n",
    "        # Default: return all patients\n",
    "        return 'MATCH (p:Patient) RETURN p ORDER BY p.patient_id LIMIT 10'\n",
    "    \n",
    "    def ask(self, question: str):\n",
    "        \"\"\"Ask a question, generate Cypher, execute it, and display results.\"\"\"\n",
    "        cypher = self.generate_cypher(question)\n",
    "        print(f'Question: {question}')\n",
    "        print(f'Generated Cypher: {cypher}')\n",
    "        print(f'Results:')\n",
    "        results = run_query(cypher)\n",
    "        for r in results:\n",
    "            print(f'  {r}')\n",
    "        return results\n",
    "\n",
    "text2cypher = Text2Cypher(LUCADAOntology())\n",
    "print('Text2Cypher engine ready. Usage: text2cypher.ask(\"your question\")')"
]))

new_cells.append(make_code_cell([
    "# Natural language queries\n",
    "text2cypher.ask('How many patients are there?')\n",
    "print()\n",
    "text2cypher.ask('What treatments are recommended for EGFR positive patients?')\n",
    "print()\n",
    "text2cypher.ask('Find patients with comorbidities')\n",
    "print()\n",
    "text2cypher.ask('Compare all patients treatments and biomarkers')"
]))

# ============================================================
# PHASE 3: Graph Visualization
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 13. Interactive Graph Visualization\n",
    "\n",
    "Visualize patient context graphs, ontology hierarchies, and decision traces using pyvis.\n",
    "Adapted from GoingMeta Session 39 (OWL to visual) and Session 15 (Semantic App)."
]))

new_cells.append(make_code_cell([
    "# Graph visualization using pyvis (interactive HTML graphs in notebook)\n",
    "try:\n",
    "    from pyvis.network import Network\n",
    "    PYVIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('Installing pyvis...')\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'pyvis', '-q'])\n",
    "    from pyvis.network import Network\n",
    "    PYVIS_AVAILABLE = True\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def visualize_graph(nodes, relationships, title='Graph', height='600px', width='100%',\n",
    "                    notebook=True, physics=True):\n",
    "    \"\"\"Create an interactive graph visualization from nodes and relationships.\"\"\"\n",
    "    net = Network(height=height, width=width, notebook=notebook, directed=True,\n",
    "                  cdn_resources='in_line')\n",
    "    \n",
    "    # Color scheme by label\n",
    "    label_colors = {\n",
    "        'Patient': '#4CAF50',\n",
    "        'Biomarker': '#FF9800',\n",
    "        'Comorbidity': '#f44336',\n",
    "        'TreatmentDecision': '#2196F3',\n",
    "        'Guideline': '#9C27B0',\n",
    "        'Diagnosis': '#00BCD4',\n",
    "        'Class': '#607D8B',\n",
    "        'Resource': '#795548',\n",
    "    }\n",
    "    \n",
    "    # Add nodes\n",
    "    for node in nodes:\n",
    "        node_id = node.get('id', '')\n",
    "        labels = node.get('labels', ['Unknown'])\n",
    "        props = node.get('properties', {})\n",
    "        primary_label = labels[0] if labels else 'Unknown'\n",
    "        color = label_colors.get(primary_label, '#666666')\n",
    "        \n",
    "        # Build display label\n",
    "        display_name = props.get('patient_id', props.get('name', props.get('marker_type',\n",
    "                       props.get('treatment', props.get('prefLabel', str(node_id)[:20])))))\n",
    "        \n",
    "        # Build tooltip\n",
    "        tooltip = f'<b>{primary_label}</b><br>'\n",
    "        for k, v in list(props.items())[:8]:\n",
    "            tooltip += f'{k}: {v}<br>'\n",
    "        \n",
    "        net.add_node(str(node_id), label=str(display_name), color=color,\n",
    "                     title=tooltip, shape='dot', size=20)\n",
    "    \n",
    "    # Add relationships\n",
    "    for rel in relationships:\n",
    "        source = str(rel.get('startNodeId', rel.get('source', '')))\n",
    "        target = str(rel.get('endNodeId', rel.get('target', '')))\n",
    "        rel_type = rel.get('type', 'RELATED')\n",
    "        net.add_edge(source, target, label=rel_type, title=rel_type)\n",
    "    \n",
    "    if physics:\n",
    "        net.set_options('{\"physics\": {\"barnesHut\": {\"gravitationalConstant\": -3000}}}')\n",
    "    \n",
    "    # Save and display\n",
    "    output_path = os.path.join(tempfile.gettempdir(), f'{title.replace(\" \", \"_\")}.html')\n",
    "    net.save_graph(output_path)\n",
    "    \n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    display(HTML(html_content))\n",
    "    print(f'Graph: {len(nodes)} nodes, {len(relationships)} relationships')\n",
    "    return net\n",
    "\n",
    "print('visualize_graph() ready.')"
]))

new_cells.append(make_md_cell([
    "### 13.1 Patient Context Graph"
]))

new_cells.append(make_code_cell([
    "# Visualize all patients and their relationships from Neo4j\n",
    "graph_query = \"\"\"\n",
    "MATCH (p:Patient)\n",
    "OPTIONAL MATCH (p)-[r1:HAS_BIOMARKER]->(b:Biomarker)\n",
    "OPTIONAL MATCH (p)-[r2:HAS_COMORBIDITY]->(c:Comorbidity)\n",
    "OPTIONAL MATCH (p)-[r3:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "WITH p, collect(DISTINCT {id: elementId(b), labels: labels(b), properties: properties(b), rel_type: 'HAS_BIOMARKER'}) as biomarkers,\n",
    "     collect(DISTINCT {id: elementId(c), labels: labels(c), properties: properties(c), rel_type: 'HAS_COMORBIDITY'}) as comorbidities,\n",
    "     collect(DISTINCT {id: elementId(d), labels: labels(d), properties: properties(d), rel_type: 'RECEIVED_RECOMMENDATION'}) as decisions\n",
    "RETURN p, elementId(p) as pid, biomarkers, comorbidities, decisions\n",
    "\"\"\"\n",
    "\n",
    "results = run_query(graph_query)\n",
    "\n",
    "nodes = []\n",
    "relationships = []\n",
    "seen_ids = set()\n",
    "\n",
    "for record in results:\n",
    "    p = record['p']\n",
    "    pid = record['pid']\n",
    "    \n",
    "    if pid not in seen_ids:\n",
    "        nodes.append({'id': pid, 'labels': ['Patient'], 'properties': dict(p)})\n",
    "        seen_ids.add(pid)\n",
    "    \n",
    "    for collection, rel_type in [\n",
    "        (record['biomarkers'], 'HAS_BIOMARKER'),\n",
    "        (record['comorbidities'], 'HAS_COMORBIDITY'),\n",
    "        (record['decisions'], 'RECEIVED_RECOMMENDATION')\n",
    "    ]:\n",
    "        for item in collection:\n",
    "            if item.get('id') and item['id'] not in seen_ids:\n",
    "                nodes.append({'id': item['id'], 'labels': item.get('labels', []), 'properties': item.get('properties', {})})\n",
    "                seen_ids.add(item['id'])\n",
    "                relationships.append({'startNodeId': pid, 'endNodeId': item['id'], 'type': rel_type})\n",
    "\n",
    "if nodes:\n",
    "    visualize_graph(nodes, relationships, title='Patient Context Graph')\n",
    "else:\n",
    "    print('No patient data found in Neo4j. Run the patient creation cells above first.')"
]))

new_cells.append(make_md_cell([
    "### 13.2 Ontology Class Hierarchy"
]))

new_cells.append(make_code_cell([
    "# Visualize LUCADA ontology as an interactive hierarchy\n",
    "from backend.src.ontology.lucada_ontology import LUCADAOntology\n",
    "\n",
    "ontology = LUCADAOntology()\n",
    "onto = ontology.create()\n",
    "\n",
    "onto_nodes = []\n",
    "onto_rels = []\n",
    "\n",
    "for cls in onto.classes():\n",
    "    onto_nodes.append({\n",
    "        'id': cls.name,\n",
    "        'labels': ['Class'],\n",
    "        'properties': {\n",
    "            'name': cls.name,\n",
    "            'prefLabel': cls.name,\n",
    "            'parents': ', '.join([p.name for p in cls.is_a if hasattr(p, 'name')])\n",
    "        }\n",
    "    })\n",
    "    for parent in cls.is_a:\n",
    "        if hasattr(parent, 'name') and parent.name != 'Thing':\n",
    "            onto_rels.append({\n",
    "                'startNodeId': cls.name,\n",
    "                'endNodeId': parent.name,\n",
    "                'type': 'SUBCLASS_OF'\n",
    "            })\n",
    "\n",
    "visualize_graph(onto_nodes, onto_rels, title='LUCADA Ontology Hierarchy')\n",
    "print(f'Ontology: {len(onto_nodes)} classes, {len(onto_rels)} subclass relationships')"
]))

new_cells.append(make_md_cell([
    "### 13.3 Decision Trace Visualization"
]))

new_cells.append(make_code_cell([
    "# Visualize a treatment decision trace for a specific patient\n",
    "decision_trace_query = \"\"\"\n",
    "MATCH (p:Patient {patient_id: 'PAT_NOTEBOOK_001'})\n",
    "OPTIONAL MATCH (p)-[r1]->(related)\n",
    "OPTIONAL MATCH (related)-[r2]->(further)\n",
    "WITH p, collect(DISTINCT {node: related, rel: type(r1), eid: elementId(related)}) as level1,\n",
    "     collect(DISTINCT {node: further, rel: type(r2), source_eid: elementId(related), eid: elementId(further)}) as level2\n",
    "RETURN p, elementId(p) as pid, level1, level2\n",
    "\"\"\"\n",
    "\n",
    "results = run_query(decision_trace_query)\n",
    "trace_nodes = []\n",
    "trace_rels = []\n",
    "seen = set()\n",
    "\n",
    "for record in results:\n",
    "    p = record['p']\n",
    "    pid = record['pid']\n",
    "    if pid not in seen:\n",
    "        trace_nodes.append({'id': pid, 'labels': ['Patient'], 'properties': dict(p)})\n",
    "        seen.add(pid)\n",
    "    \n",
    "    for item in record['level1']:\n",
    "        eid = item['eid']\n",
    "        if eid and eid not in seen:\n",
    "            node = item['node']\n",
    "            trace_nodes.append({'id': eid, 'labels': list(node.labels) if hasattr(node, 'labels') else ['Node'], 'properties': dict(node)})\n",
    "            seen.add(eid)\n",
    "            trace_rels.append({'startNodeId': pid, 'endNodeId': eid, 'type': item['rel']})\n",
    "    \n",
    "    for item in record['level2']:\n",
    "        eid = item.get('eid')\n",
    "        source = item.get('source_eid')\n",
    "        if eid and eid not in seen:\n",
    "            node = item['node']\n",
    "            if node:\n",
    "                trace_nodes.append({'id': eid, 'labels': list(node.labels) if hasattr(node, 'labels') else ['Node'], 'properties': dict(node)})\n",
    "                seen.add(eid)\n",
    "                if source:\n",
    "                    trace_rels.append({'startNodeId': source, 'endNodeId': eid, 'type': item.get('rel', 'RELATED')})\n",
    "\n",
    "if trace_nodes:\n",
    "    visualize_graph(trace_nodes, trace_rels, title='Decision Trace - PAT_NOTEBOOK_001')\n",
    "else:\n",
    "    print('No decision trace data found. Run the patient creation and treatment update cells first.')"
]))

# ============================================================
# PHASE 4: Provenance & Decision Trace
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 14. Provenance Tracking & Decision Trace\n",
    "\n",
    "Track the full reasoning chain: data source -> agent execution -> guideline match -> recommendation.\n",
    "Implements W3C PROV-DM (Provenance Data Model) patterns."
]))

new_cells.append(make_code_cell([
    "# Initialize provenance tracker\n",
    "from backend.src.db.provenance_tracker import ProvenanceTracker, ProvenanceType, ProvenanceRelation\n",
    "from backend.src.db.provenance_tracker import ProvenanceEntity, ProvenanceActivity\n",
    "\n",
    "provenance = ProvenanceTracker()\n",
    "\n",
    "# Record a complete provenance chain for a patient analysis\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Entity: Patient data source\n",
    "patient_entity = ProvenanceEntity(\n",
    "    id='prov:patient_001_data',\n",
    "    type=ProvenanceType.ENTITY,\n",
    "    label='Patient PAT_NOTEBOOK_001 Clinical Data',\n",
    "    timestamp=datetime.now(),\n",
    "    attributes={\n",
    "        'patient_id': 'PAT_NOTEBOOK_001',\n",
    "        'data_source': 'notebook_input',\n",
    "        'age': 68, 'stage': 'IIIA', 'histology': 'adenocarcinoma'\n",
    "    }\n",
    ")\n",
    "\n",
    "# 2. Activity: Agent pipeline execution\n",
    "pipeline_activity = ProvenanceActivity(\n",
    "    id='prov:pipeline_run_001',\n",
    "    type=ProvenanceType.ACTIVITY,\n",
    "    label='6-Agent LCA Pipeline Execution',\n",
    "    timestamp=datetime.now(),\n",
    "    attributes={\n",
    "        'agents': ['IngestionAgent', 'SemanticMappingAgent', 'ClassificationAgent',\n",
    "                   'ConflictResolutionAgent', 'PersistenceAgent', 'ExplanationAgent'],\n",
    "        'workflow_type': 'standard',\n",
    "        'execution_time_ms': 1250\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3. Entity: Treatment recommendation (output)\n",
    "recommendation_entity = ProvenanceEntity(\n",
    "    id='prov:recommendation_001',\n",
    "    type=ProvenanceType.ENTITY,\n",
    "    label='Osimertinib Recommendation',\n",
    "    timestamp=datetime.now(),\n",
    "    attributes={\n",
    "        'treatment': 'Osimertinib (EGFR TKI)',\n",
    "        'evidence_level': 'Grade A',\n",
    "        'confidence': 0.92,\n",
    "        'guideline': 'NICE CG121 R8A'\n",
    "    },\n",
    "    derived_from=['prov:patient_001_data'],\n",
    "    generated_by='prov:pipeline_run_001'\n",
    ")\n",
    "\n",
    "# Register entities\n",
    "provenance.register_entity(patient_entity)\n",
    "provenance.register_entity(pipeline_activity)\n",
    "provenance.register_entity(recommendation_entity)\n",
    "\n",
    "# Record relationships\n",
    "provenance.record_relation(\n",
    "    ProvenanceRelation.WAS_GENERATED_BY,\n",
    "    'prov:recommendation_001',\n",
    "    'prov:pipeline_run_001'\n",
    ")\n",
    "provenance.record_relation(\n",
    "    ProvenanceRelation.USED,\n",
    "    'prov:pipeline_run_001',\n",
    "    'prov:patient_001_data'\n",
    ")\n",
    "provenance.record_relation(\n",
    "    ProvenanceRelation.WAS_DERIVED_FROM,\n",
    "    'prov:recommendation_001',\n",
    "    'prov:patient_001_data'\n",
    ")\n",
    "\n",
    "print('Provenance chain recorded:')\n",
    "print(f'  Entities: {len(provenance.entities)}')\n",
    "print(f'  Relations: {len(provenance.relations)}')\n",
    "print(f'\\nProvenance for recommendation:')\n",
    "rec_prov = provenance.get_provenance('prov:recommendation_001')\n",
    "print(f'  Entity: {rec_prov[\"label\"]}')\n",
    "print(f'  Generated by: {rec_prov.get(\"generated_by\", \"N/A\")}')\n",
    "print(f'  Derived from: {rec_prov.get(\"derived_from\", [])}')"
]))

new_cells.append(make_code_cell([
    "# Persist provenance to Neo4j\n",
    "prov_create_query = \"\"\"\n",
    "// Create provenance nodes\n",
    "MERGE (src:ProvenanceEntity {prov_id: $source_id})\n",
    "SET src.label = $source_label, src.type = 'entity', src.attributes = $source_attrs\n",
    "\n",
    "MERGE (act:ProvenanceActivity {prov_id: $activity_id})\n",
    "SET act.label = $activity_label, act.type = 'activity', act.attributes = $activity_attrs\n",
    "\n",
    "MERGE (out:ProvenanceEntity {prov_id: $output_id})\n",
    "SET out.label = $output_label, out.type = 'entity', out.attributes = $output_attrs\n",
    "\n",
    "// Create PROV relationships\n",
    "MERGE (out)-[:WAS_GENERATED_BY]->(act)\n",
    "MERGE (act)-[:USED]->(src)\n",
    "MERGE (out)-[:WAS_DERIVED_FROM]->(src)\n",
    "\n",
    "// Link to patient\n",
    "WITH src, act, out\n",
    "OPTIONAL MATCH (p:Patient {patient_id: $patient_id})\n",
    "FOREACH (_ IN CASE WHEN p IS NOT NULL THEN [1] ELSE [] END |\n",
    "    MERGE (src)-[:ABOUT_PATIENT]->(p)\n",
    "    MERGE (out)-[:RECOMMENDATION_FOR]->(p)\n",
    ")\n",
    "RETURN src.prov_id, act.prov_id, out.prov_id\n",
    "\"\"\"\n",
    "\n",
    "import json as _json\n",
    "result = run_query(prov_create_query, {\n",
    "    'source_id': 'prov:patient_001_data',\n",
    "    'source_label': 'Patient PAT_NOTEBOOK_001 Clinical Data',\n",
    "    'source_attrs': _json.dumps({'patient_id': 'PAT_NOTEBOOK_001', 'data_source': 'notebook'}),\n",
    "    'activity_id': 'prov:pipeline_run_001',\n",
    "    'activity_label': '6-Agent LCA Pipeline',\n",
    "    'activity_attrs': _json.dumps({'agents': 6, 'workflow': 'standard'}),\n",
    "    'output_id': 'prov:recommendation_001',\n",
    "    'output_label': 'Osimertinib Recommendation',\n",
    "    'output_attrs': _json.dumps({'treatment': 'Osimertinib', 'evidence': 'Grade A', 'confidence': 0.92}),\n",
    "    'patient_id': 'PAT_NOTEBOOK_001'\n",
    "})\n",
    "print(f'Provenance persisted to Neo4j: {result}')"
]))

new_cells.append(make_code_cell([
    "# Query provenance: \"Show me the reasoning chain for this recommendation\"\n",
    "prov_query = \"\"\"\n",
    "MATCH (rec:ProvenanceEntity {prov_id: 'prov:recommendation_001'})\n",
    "OPTIONAL MATCH (rec)-[:WAS_GENERATED_BY]->(act:ProvenanceActivity)\n",
    "OPTIONAL MATCH (act)-[:USED]->(src:ProvenanceEntity)\n",
    "OPTIONAL MATCH (rec)-[:WAS_DERIVED_FROM]->(origin)\n",
    "RETURN rec.label as recommendation, rec.attributes as rec_details,\n",
    "       act.label as activity, act.attributes as act_details,\n",
    "       src.label as source, src.attributes as src_details\n",
    "\"\"\"\n",
    "\n",
    "print('=== Provenance Chain Query ===')\n",
    "results = run_query(prov_query)\n",
    "for r in results:\n",
    "    print(f'\\n  Source: {r[\"source\"]}')\n",
    "    print(f'  Activity: {r[\"activity\"]}')\n",
    "    print(f'  Recommendation: {r[\"recommendation\"]}')\n",
    "    if r.get('rec_details'):\n",
    "        details = _json.loads(r['rec_details']) if isinstance(r['rec_details'], str) else r['rec_details']\n",
    "        print(f'  Details: {details}')"
]))

# ============================================================
# PHASE 5: Context Graph & Grounded Citations
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 15. Context Graph & Grounded Citations\n",
    "\n",
    "Build patient context subgraphs and provide grounded citations linking\n",
    "every recommendation to its evidence source."
]))

new_cells.append(make_code_cell([
    "# Context Graph Client\n",
    "from backend.src.db.context_graph_client import LCAContextGraphClient\n",
    "\n",
    "graph_client = LCAContextGraphClient(\n",
    "    uri=NEO4J_URI, username=NEO4J_USER,\n",
    "    password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "# Get full context graph for a patient\n",
    "graph_data = graph_client.get_graph_data(\n",
    "    center_node_id='PAT_NOTEBOOK_001',\n",
    "    depth=3,\n",
    "    limit=100\n",
    ")\n",
    "\n",
    "print(f'Context Graph for PAT_NOTEBOOK_001:')\n",
    "print(f'  Nodes: {len(graph_data.nodes)}')\n",
    "print(f'  Relationships: {len(graph_data.relationships)}')\n",
    "\n",
    "# Visualize with pyvis\n",
    "ctx_nodes = [{'id': n.id, 'labels': n.labels, 'properties': n.properties} for n in graph_data.nodes]\n",
    "ctx_rels = [{'startNodeId': r.startNodeId, 'endNodeId': r.endNodeId, 'type': r.type} for r in graph_data.relationships]\n",
    "\n",
    "if ctx_nodes:\n",
    "    visualize_graph(ctx_nodes, ctx_rels, title='Context Graph - PAT_NOTEBOOK_001')"
]))

new_cells.append(make_code_cell([
    "# Grounded Citations: Link every recommendation to its evidence source\n",
    "\n",
    "class GroundedCitationBuilder:\n",
    "    \"\"\"Build citations that ground recommendations in evidence.\"\"\"\n",
    "    \n",
    "    def __init__(self, driver, database):\n",
    "        self.driver = driver\n",
    "        self.database = database\n",
    "    \n",
    "    def get_citations_for_patient(self, patient_id: str) -> list:\n",
    "        \"\"\"Get all grounded citations for a patient's recommendations.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Patient {patient_id: $patient_id})-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "        OPTIONAL MATCH (d)-[:APPLIED_GUIDELINE]->(g:Guideline)\n",
    "        OPTIONAL MATCH (d)<-[:WAS_GENERATED_BY]-(prov:ProvenanceEntity)\n",
    "        OPTIONAL MATCH (prov)-[:WAS_DERIVED_FROM]->(src)\n",
    "        RETURN d.treatment as treatment, d.evidence_level as evidence,\n",
    "               d.confidence_score as confidence, d.reasoning as reasoning,\n",
    "               g.name as guideline, g.source as guideline_source,\n",
    "               prov.label as provenance, src.label as data_source\n",
    "        \"\"\"\n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            results = session.run(query, patient_id=patient_id)\n",
    "            citations = []\n",
    "            for record in results:\n",
    "                citations.append({\n",
    "                    'treatment': record['treatment'],\n",
    "                    'evidence_level': record['evidence'],\n",
    "                    'confidence': record['confidence'],\n",
    "                    'reasoning': record['reasoning'],\n",
    "                    'guideline': record.get('guideline', 'N/A'),\n",
    "                    'guideline_source': record.get('guideline_source', 'N/A'),\n",
    "                    'provenance': record.get('provenance', 'N/A'),\n",
    "                    'data_source': record.get('data_source', 'N/A')\n",
    "                })\n",
    "            return citations\n",
    "    \n",
    "    def format_citations(self, citations: list) -> str:\n",
    "        \"\"\"Format citations as a readable markdown report.\"\"\"\n",
    "        if not citations:\n",
    "            return 'No citations found.'\n",
    "        \n",
    "        text = '## Grounded Citations Report\\n\\n'\n",
    "        for i, c in enumerate(citations, 1):\n",
    "            text += f'### Citation {i}: {c[\"treatment\"]}\\n\\n'\n",
    "            text += f'| Field | Value |\\n|---|---|\\n'\n",
    "            text += f'| Treatment | {c[\"treatment\"]} |\\n'\n",
    "            text += f'| Evidence Level | {c[\"evidence_level\"]} |\\n'\n",
    "            text += f'| Confidence | {c[\"confidence\"]} |\\n'\n",
    "            text += f'| Guideline | {c[\"guideline\"]} |\\n'\n",
    "            text += f'| Data Source | {c[\"data_source\"]} |\\n'\n",
    "            text += f'| Provenance | {c[\"provenance\"]} |\\n\\n'\n",
    "            if c.get('reasoning'):\n",
    "                text += f'**Reasoning:** {c[\"reasoning\"]}\\n\\n'\n",
    "        return text\n",
    "\n",
    "citation_builder = GroundedCitationBuilder(driver, NEO4J_DATABASE)\n",
    "citations = citation_builder.get_citations_for_patient('PAT_NOTEBOOK_001')\n",
    "print(citation_builder.format_citations(citations))"
]))

# ============================================================
# PHASE 6: GoingMeta Pattern Integration
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 16. GoingMeta Pattern Integration\n",
    "\n",
    "Applying patterns from the GoingMeta series to enhance CoherencePLM:\n",
    "- SHACL validation (Session 3/11)\n",
    "- Ontology-driven RAG (Session 24)\n",
    "- Semantic similarity (Session 16/21)\n",
    "- Ontology quality evaluation (Session 42)\n",
    "- Ontology visualization (Session 39)"
]))

new_cells.append(make_md_cell([
    "### 16.1 SHACL-style Data Validation\n",
    "\n",
    "Validate clinical data against ontology constraints. Adapted from GoingMeta Session 3."
]))

new_cells.append(make_code_cell([
    "# SHACL-style validation for clinical patient data\n",
    "class ClinicalDataValidator:\n",
    "    \"\"\"Validate patient data against clinical ontology constraints.\n",
    "    Inspired by GoingMeta Session 3: SHACL graph shapes.\"\"\"\n",
    "    \n",
    "    # Define validation shapes (SHACL-inspired)\n",
    "    PATIENT_SHAPE = {\n",
    "        'required_fields': ['age', 'sex', 'tnm_stage', 'histology_type'],\n",
    "        'constraints': {\n",
    "            'age': {'type': int, 'min': 1, 'max': 120},\n",
    "            'sex': {'type': str, 'values': ['M', 'F', 'U']},\n",
    "            'tnm_stage': {'type': str, 'pattern': r'(I{1,3}[ABC]?|IV|Limited|Extensive)'},\n",
    "            'histology_type': {'type': str, 'values': [\n",
    "                'adenocarcinoma', 'squamous_cell', 'large_cell', 'small_cell',\n",
    "                'nsclc_nos', 'carcinosarcoma'\n",
    "            ]},\n",
    "            'performance_status': {'type': int, 'min': 0, 'max': 4},\n",
    "        },\n",
    "        'biomarker_constraints': {\n",
    "            'EGFR': {'valid_values': ['Positive', 'Negative', 'Exon 19 deletion', 'Exon 21 L858R', 'T790M']},\n",
    "            'ALK': {'valid_values': ['Positive', 'Negative']},\n",
    "            'PD-L1': {'valid_pattern': r'\\d+%?'},\n",
    "            'KRAS': {'valid_values': ['Positive', 'Negative', 'G12C', 'G12V', 'G12D']},\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def validate_patient(self, patient_data: dict) -> dict:\n",
    "        \"\"\"Validate patient data and return violations.\"\"\"\n",
    "        import re\n",
    "        violations = []\n",
    "        warnings = []\n",
    "        \n",
    "        # Check required fields\n",
    "        for field in self.PATIENT_SHAPE['required_fields']:\n",
    "            if field not in patient_data or patient_data[field] is None:\n",
    "                violations.append(f'Missing required field: {field}')\n",
    "        \n",
    "        # Check constraints\n",
    "        for field, constraint in self.PATIENT_SHAPE['constraints'].items():\n",
    "            value = patient_data.get(field)\n",
    "            if value is None:\n",
    "                continue\n",
    "            \n",
    "            if 'values' in constraint and str(value).lower() not in [v.lower() for v in constraint['values']]:\n",
    "                violations.append(f'{field}: \"{value}\" not in allowed values {constraint[\"values\"]}')\n",
    "            \n",
    "            if 'min' in constraint and isinstance(value, (int, float)) and value < constraint['min']:\n",
    "                violations.append(f'{field}: {value} below minimum {constraint[\"min\"]}')\n",
    "            \n",
    "            if 'max' in constraint and isinstance(value, (int, float)) and value > constraint['max']:\n",
    "                violations.append(f'{field}: {value} above maximum {constraint[\"max\"]}')\n",
    "            \n",
    "            if 'pattern' in constraint and isinstance(value, str):\n",
    "                if not re.match(constraint['pattern'], value, re.IGNORECASE):\n",
    "                    violations.append(f'{field}: \"{value}\" does not match pattern {constraint[\"pattern\"]}')\n",
    "        \n",
    "        # Validate biomarkers\n",
    "        biomarkers = patient_data.get('biomarkers', [])\n",
    "        for bm in biomarkers:\n",
    "            bm_type = bm.get('type', '')\n",
    "            bm_value = bm.get('value', '')\n",
    "            if bm_type in self.PATIENT_SHAPE['biomarker_constraints']:\n",
    "                bc = self.PATIENT_SHAPE['biomarker_constraints'][bm_type]\n",
    "                if 'valid_values' in bc and bm_value not in bc['valid_values']:\n",
    "                    warnings.append(f'Biomarker {bm_type}: \"{bm_value}\" not in expected values')\n",
    "        \n",
    "        is_valid = len(violations) == 0\n",
    "        return {\n",
    "            'valid': is_valid,\n",
    "            'violations': violations,\n",
    "            'warnings': warnings,\n",
    "            'fields_checked': len(self.PATIENT_SHAPE['constraints'])\n",
    "        }\n",
    "\n",
    "validator = ClinicalDataValidator()\n",
    "\n",
    "# Test validation\n",
    "test_cases = [\n",
    "    {'name': 'Valid', 'data': {'age': 68, 'sex': 'M', 'tnm_stage': 'IIIA', 'histology_type': 'adenocarcinoma', 'performance_status': 1}},\n",
    "    {'name': 'Missing stage', 'data': {'age': 68, 'sex': 'M', 'histology_type': 'adenocarcinoma'}},\n",
    "    {'name': 'Invalid age', 'data': {'age': 200, 'sex': 'M', 'tnm_stage': 'IV', 'histology_type': 'adenocarcinoma'}},\n",
    "    {'name': 'Bad histology', 'data': {'age': 55, 'sex': 'F', 'tnm_stage': 'II', 'histology_type': 'unknown_type'}},\n",
    "]\n",
    "\n",
    "for tc in test_cases:\n",
    "    result = validator.validate_patient(tc['data'])\n",
    "    status = 'PASS' if result['valid'] else 'FAIL'\n",
    "    print(f'[{status}] {tc[\"name\"]}: {result[\"violations\"] if result[\"violations\"] else \"OK\"}')\n",
    "    if result['warnings']:\n",
    "        print(f'  Warnings: {result[\"warnings\"]}')"
]))

new_cells.append(make_md_cell([
    "### 16.2 Ontology-Driven RAG\n",
    "\n",
    "Use the LUCADA ontology to dynamically guide retrieval-augmented generation.\n",
    "Adapted from GoingMeta Session 24."
]))

new_cells.append(make_code_cell([
    "# Ontology-Driven RAG: Use ontology structure to enhance retrieval\n",
    "class OntologyDrivenRAG:\n",
    "    \"\"\"RAG enhanced with ontology structure for better clinical Q&A.\n",
    "    Adapted from GoingMeta Session 24: Ontology-driven RAG patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, driver, database, ontology):\n",
    "        self.driver = driver\n",
    "        self.database = database\n",
    "        self.ontology = ontology\n",
    "        self.onto = ontology.create()\n",
    "        self._build_concept_index()\n",
    "    \n",
    "    def _build_concept_index(self):\n",
    "        \"\"\"Build index of ontology concepts for concept extraction.\"\"\"\n",
    "        self.concepts = {}\n",
    "        for cls in self.onto.classes():\n",
    "            name = cls.name.lower()\n",
    "            self.concepts[name] = {\n",
    "                'class_name': cls.name,\n",
    "                'parents': [p.name for p in cls.is_a if hasattr(p, 'name')],\n",
    "                'children': [c.name for c in cls.subclasses() if c != cls]\n",
    "            }\n",
    "        # Add common aliases\n",
    "        self.aliases = {\n",
    "            'egfr': 'EGFRMutation', 'alk': 'ALKRearrangement',\n",
    "            'pd-l1': 'PDL1Expression', 'pdl1': 'PDL1Expression',\n",
    "            'nsclc': 'NonSmallCellCarcinoma', 'sclc': 'SmallCellCarcinoma',\n",
    "            'chemo': 'Chemotherapy', 'immunotherapy': 'Immunotherapy',\n",
    "            'surgery': 'Surgery', 'radiation': 'Radiotherapy',\n",
    "            'copd': 'COPD', 'diabetes': 'Diabetes',\n",
    "        }\n",
    "    \n",
    "    def extract_concepts(self, question: str) -> list:\n",
    "        \"\"\"Extract ontology concepts from a natural language question.\"\"\"\n",
    "        q = question.lower()\n",
    "        found = []\n",
    "        for alias, concept in self.aliases.items():\n",
    "            if alias in q:\n",
    "                found.append(concept)\n",
    "        for concept_name in self.concepts:\n",
    "            if concept_name in q and concept_name not in [f.lower() for f in found]:\n",
    "                found.append(self.concepts[concept_name]['class_name'])\n",
    "        return found\n",
    "    \n",
    "    def get_related_concepts(self, concepts: list) -> list:\n",
    "        \"\"\"Expand concepts using ontology hierarchy (parent/child/sibling).\"\"\"\n",
    "        expanded = set(concepts)\n",
    "        for c in concepts:\n",
    "            c_lower = c.lower()\n",
    "            if c_lower in self.concepts:\n",
    "                expanded.update(self.concepts[c_lower].get('parents', []))\n",
    "                expanded.update(self.concepts[c_lower].get('children', [])[:3])  # Limit children\n",
    "        return list(expanded)\n",
    "    \n",
    "    def retrieve_context(self, question: str) -> dict:\n",
    "        \"\"\"Retrieve relevant context from the graph based on extracted concepts.\"\"\"\n",
    "        concepts = self.extract_concepts(question)\n",
    "        expanded = self.get_related_concepts(concepts)\n",
    "        \n",
    "        # Query graph for relevant data based on concepts\n",
    "        context_parts = []\n",
    "        \n",
    "        # Check for biomarker-related concepts\n",
    "        biomarker_concepts = [c for c in expanded if 'Mutation' in c or 'Expression' in c or 'Rearrangement' in c]\n",
    "        if biomarker_concepts:\n",
    "            query = \"\"\"MATCH (p:Patient)-[:HAS_BIOMARKER]->(b:Biomarker)\n",
    "            OPTIONAL MATCH (p)-[:RECEIVED_RECOMMENDATION]->(d:TreatmentDecision)\n",
    "            RETURN p.patient_id as patient, b.marker_type as biomarker, b.value as value,\n",
    "                   d.treatment as treatment, d.evidence_level as evidence LIMIT 10\"\"\"\n",
    "            with self.driver.session(database=self.database) as session:\n",
    "                results = [dict(r) for r in session.run(query)]\n",
    "                if results:\n",
    "                    context_parts.append({'source': 'patient_biomarkers', 'data': results})\n",
    "        \n",
    "        # Check for treatment concepts\n",
    "        treatment_concepts = [c for c in expanded if any(x in c for x in ['Therapy', 'Surgery', 'Chemotherapy', 'Immunotherapy', 'Radiotherapy'])]\n",
    "        if treatment_concepts:\n",
    "            query = \"\"\"MATCH (d:TreatmentDecision)\n",
    "            RETURN d.treatment as treatment, d.evidence_level as evidence,\n",
    "                   d.confidence_score as confidence, d.reasoning as reasoning LIMIT 10\"\"\"\n",
    "            with self.driver.session(database=self.database) as session:\n",
    "                results = [dict(r) for r in session.run(query)]\n",
    "                if results:\n",
    "                    context_parts.append({'source': 'treatment_decisions', 'data': results})\n",
    "        \n",
    "        return {\n",
    "            'question': question,\n",
    "            'extracted_concepts': concepts,\n",
    "            'expanded_concepts': expanded,\n",
    "            'context': context_parts,\n",
    "            'concept_count': len(expanded)\n",
    "        }\n",
    "\n",
    "rag = OntologyDrivenRAG(driver, NEO4J_DATABASE, LUCADAOntology())\n",
    "\n",
    "# Test concept extraction and retrieval\n",
    "questions = [\n",
    "    'What treatment is recommended for EGFR positive NSCLC?',\n",
    "    'How does PD-L1 expression affect immunotherapy selection?',\n",
    "    'What are the surgery options for early stage adenocarcinoma?',\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    result = rag.retrieve_context(q)\n",
    "    print(f'\\nQ: {q}')\n",
    "    print(f'  Concepts: {result[\"extracted_concepts\"]}')\n",
    "    print(f'  Expanded: {result[\"expanded_concepts\"]}')\n",
    "    print(f'  Context sources: {len(result[\"context\"])}')\n",
    "    for ctx in result['context']:\n",
    "        print(f'    {ctx[\"source\"]}: {len(ctx[\"data\"])} records')"
]))

new_cells.append(make_md_cell([
    "### 16.3 Semantic Patient Similarity\n",
    "\n",
    "Graph-based semantic similarity between patients using ontology taxonomy.\n",
    "Adapted from GoingMeta Sessions 16 and 21."
]))

new_cells.append(make_code_cell([
    "# Semantic Patient Similarity using graph structure\n",
    "class PatientSimilarityEngine:\n",
    "    \"\"\"Compute patient similarity using graph-based features.\n",
    "    Adapted from GoingMeta Session 16: Semantic Similarity Metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, driver, database):\n",
    "        self.driver = driver\n",
    "        self.database = database\n",
    "    \n",
    "    def get_patient_features(self, patient_id: str) -> dict:\n",
    "        \"\"\"Extract feature vector for a patient from the graph.\"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Patient {patient_id: $patient_id})\n",
    "        OPTIONAL MATCH (p)-[:HAS_BIOMARKER]->(b:Biomarker)\n",
    "        OPTIONAL MATCH (p)-[:HAS_COMORBIDITY]->(c:Comorbidity)\n",
    "        RETURN p, collect(DISTINCT {type: b.marker_type, value: b.value, status: b.status}) as biomarkers,\n",
    "               collect(DISTINCT c.name) as comorbidities\n",
    "        \"\"\"\n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            result = session.run(query, patient_id=patient_id)\n",
    "            record = result.single()\n",
    "            if not record:\n",
    "                return None\n",
    "            p = dict(record['p'])\n",
    "            return {\n",
    "                'patient_id': patient_id,\n",
    "                'age': p.get('age', 0),\n",
    "                'sex': p.get('sex', 'U'),\n",
    "                'stage': p.get('tnm_stage', ''),\n",
    "                'histology': p.get('histology_type', ''),\n",
    "                'ps': p.get('performance_status', 0),\n",
    "                'biomarkers': {b['type']: b['status'] for b in record['biomarkers'] if b.get('type')},\n",
    "                'comorbidities': set(c for c in record['comorbidities'] if c)\n",
    "            }\n",
    "    \n",
    "    def compute_similarity(self, patient_a: dict, patient_b: dict) -> dict:\n",
    "        \"\"\"Compute multi-dimensional similarity between two patients.\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        # Age similarity (Gaussian)\n",
    "        import math\n",
    "        age_diff = abs(patient_a['age'] - patient_b['age'])\n",
    "        scores['age'] = math.exp(-(age_diff**2) / (2 * 10**2))  # sigma=10\n",
    "        \n",
    "        # Stage similarity\n",
    "        stage_map = {'I': 1, 'IA': 1, 'IB': 1.5, 'II': 2, 'IIA': 2, 'IIB': 2.5,\n",
    "                     'IIIA': 3, 'IIIB': 3.5, 'IV': 4, 'Limited': 3, 'Extensive': 4}\n",
    "        sa = stage_map.get(patient_a['stage'], 2.5)\n",
    "        sb = stage_map.get(patient_b['stage'], 2.5)\n",
    "        scores['stage'] = 1.0 - abs(sa - sb) / 4.0\n",
    "        \n",
    "        # Histology match\n",
    "        scores['histology'] = 1.0 if patient_a['histology'] == patient_b['histology'] else 0.0\n",
    "        \n",
    "        # Sex match\n",
    "        scores['sex'] = 1.0 if patient_a['sex'] == patient_b['sex'] else 0.5\n",
    "        \n",
    "        # Biomarker similarity (Jaccard)\n",
    "        bm_a = set(patient_a['biomarkers'].keys())\n",
    "        bm_b = set(patient_b['biomarkers'].keys())\n",
    "        if bm_a or bm_b:\n",
    "            intersection = bm_a & bm_b\n",
    "            union = bm_a | bm_b\n",
    "            jaccard = len(intersection) / len(union) if union else 0\n",
    "            # Bonus for matching biomarker status\n",
    "            status_match = sum(1 for b in intersection if patient_a['biomarkers'].get(b) == patient_b['biomarkers'].get(b))\n",
    "            scores['biomarkers'] = (jaccard + (status_match / len(union) if union else 0)) / 2\n",
    "        else:\n",
    "            scores['biomarkers'] = 1.0\n",
    "        \n",
    "        # Comorbidity similarity (Jaccard)\n",
    "        cm_a = patient_a['comorbidities']\n",
    "        cm_b = patient_b['comorbidities']\n",
    "        if cm_a or cm_b:\n",
    "            scores['comorbidities'] = len(cm_a & cm_b) / len(cm_a | cm_b)\n",
    "        else:\n",
    "            scores['comorbidities'] = 1.0\n",
    "        \n",
    "        # Weighted overall score\n",
    "        weights = {'age': 0.1, 'stage': 0.25, 'histology': 0.25, 'sex': 0.05, 'biomarkers': 0.25, 'comorbidities': 0.1}\n",
    "        overall = sum(scores[k] * weights[k] for k in weights)\n",
    "        scores['overall'] = round(overall, 3)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def find_similar_patients(self, patient_id: str, top_k: int = 5) -> list:\n",
    "        \"\"\"Find the most similar patients in the database.\"\"\"\n",
    "        target = self.get_patient_features(patient_id)\n",
    "        if not target:\n",
    "            return []\n",
    "        \n",
    "        # Get all other patients\n",
    "        query = 'MATCH (p:Patient) WHERE p.patient_id <> $pid RETURN p.patient_id as pid'\n",
    "        with self.driver.session(database=self.database) as session:\n",
    "            other_ids = [r['pid'] for r in session.run(query, pid=patient_id)]\n",
    "        \n",
    "        similarities = []\n",
    "        for pid in other_ids:\n",
    "            other = self.get_patient_features(pid)\n",
    "            if other:\n",
    "                sim = self.compute_similarity(target, other)\n",
    "                similarities.append({'patient_id': pid, **sim})\n",
    "        \n",
    "        similarities.sort(key=lambda x: x['overall'], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "\n",
    "sim_engine = PatientSimilarityEngine(driver, NEO4J_DATABASE)\n",
    "\n",
    "# Find patients similar to PAT_NOTEBOOK_001\n",
    "similar = sim_engine.find_similar_patients('PAT_NOTEBOOK_001')\n",
    "print('=== Patients Similar to PAT_NOTEBOOK_001 ===')\n",
    "for s in similar:\n",
    "    print(f'  {s[\"patient_id\"]}: Overall={s[\"overall\"]:.3f} '\n",
    "          f'(stage={s[\"stage\"]:.2f}, histology={s[\"histology\"]:.2f}, biomarkers={s[\"biomarkers\"]:.2f})')"
]))

new_cells.append(make_md_cell([
    "### 16.4 Ontology Quality Evaluation\n",
    "\n",
    "Evaluate LUCADA ontology completeness against competency questions.\n",
    "Adapted from GoingMeta Session 42."
]))

new_cells.append(make_code_cell([
    "# Ontology Quality Evaluation against Competency Questions\n",
    "# Adapted from GoingMeta Session 42\n",
    "\n",
    "class OntologyQualityEvaluator:\n",
    "    \"\"\"Evaluate ontology fitness against competency questions.\"\"\"\n",
    "    \n",
    "    COMPETENCY_QUESTIONS = [\n",
    "        'Can we determine the treatment pathway based on TNM staging?',\n",
    "        'Can we identify patients by biomarker profile (EGFR, ALK, PD-L1)?',\n",
    "        'Can we assess comorbidity impact on treatment selection?',\n",
    "        'Can we trace the reasoning chain from patient data to recommendation?',\n",
    "        'Can we compare treatments by evidence level?',\n",
    "        'Can we identify drug interactions for a medication regimen?',\n",
    "        'Can we find similar historical cases?',\n",
    "        'Can we track ontology version changes over time?',\n",
    "        'Can we validate data quality against clinical standards?',\n",
    "        'Can we generate natural language explanations of decisions?',\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, ontology):\n",
    "        self.onto = ontology.create()\n",
    "        self.class_names = {cls.name.lower(): cls.name for cls in self.onto.classes()}\n",
    "        self.properties = {prop.name.lower(): prop.name for prop in self.onto.properties()}\n",
    "    \n",
    "    def evaluate(self) -> list:\n",
    "        \"\"\"Evaluate each competency question.\"\"\"\n",
    "        results = []\n",
    "        checks = [\n",
    "            self._check_staging,\n",
    "            self._check_biomarkers,\n",
    "            self._check_comorbidities,\n",
    "            self._check_reasoning_trace,\n",
    "            self._check_evidence_levels,\n",
    "            self._check_drug_interactions,\n",
    "            self._check_similar_cases,\n",
    "            self._check_versioning,\n",
    "            self._check_data_quality,\n",
    "            self._check_nl_generation,\n",
    "        ]\n",
    "        \n",
    "        for i, (cq, check_fn) in enumerate(zip(self.COMPETENCY_QUESTIONS, checks)):\n",
    "            score, reason = check_fn()\n",
    "            results.append({\n",
    "                'cq': cq,\n",
    "                'score': score,\n",
    "                'reason': reason\n",
    "            })\n",
    "        return results\n",
    "    \n",
    "    def _has_class(self, *names):\n",
    "        return all(n.lower() in self.class_names for n in names)\n",
    "    \n",
    "    def _check_staging(self):\n",
    "        if self._has_class('TNMStaging', 'TStage', 'NStage', 'MStage'):\n",
    "            return 1.0, 'TNMStaging hierarchy fully modeled with T/N/M components'\n",
    "        return 0.5, 'Partial staging support'\n",
    "    \n",
    "    def _check_biomarkers(self):\n",
    "        markers = ['EGFRMutation', 'ALKRearrangement', 'PDL1Expression', 'KRASMutation', 'ROSRearrangement']\n",
    "        found = sum(1 for m in markers if m.lower() in self.class_names)\n",
    "        return found / len(markers), f'{found}/{len(markers)} biomarker classes present'\n",
    "    \n",
    "    def _check_comorbidities(self):\n",
    "        if self._has_class('Comorbidity', 'COPD', 'Diabetes'):\n",
    "            return 1.0, 'Comorbidity hierarchy modeled with specific conditions'\n",
    "        return 0.5, 'Partial comorbidity support'\n",
    "    \n",
    "    def _check_reasoning_trace(self):\n",
    "        if self._has_class('Argumentation', 'Decision', 'Intent', 'Argument'):\n",
    "            return 1.0, 'Argumentation framework classes present'\n",
    "        return 0.3, 'Limited argumentation support'\n",
    "    \n",
    "    def _check_evidence_levels(self):\n",
    "        if self._has_class('TreatmentPlan'):\n",
    "            return 0.7, 'TreatmentPlan exists but evidence level is a property, not a class'\n",
    "        return 0.3, 'Limited evidence level modeling'\n",
    "    \n",
    "    def _check_drug_interactions(self):\n",
    "        return 0.0, 'No drug interaction classes in ontology - needs extension'\n",
    "    \n",
    "    def _check_similar_cases(self):\n",
    "        if self._has_class('PatientScenario', 'Patient'):\n",
    "            return 0.8, 'PatientScenario and Patient classes enable case comparison'\n",
    "        return 0.3, 'Limited case comparison support'\n",
    "    \n",
    "    def _check_versioning(self):\n",
    "        return 0.0, 'No ontology versioning classes - needs GoingMeta S19 pattern'\n",
    "    \n",
    "    def _check_data_quality(self):\n",
    "        return 0.2, 'No SHACL shapes defined - needs GoingMeta S3/S11 pattern'\n",
    "    \n",
    "    def _check_nl_generation(self):\n",
    "        if self._has_class('Argumentation', 'Outcome'):\n",
    "            return 0.6, 'Argumentation + Outcome support NL generation but no annotations'\n",
    "        return 0.2, 'Limited NL generation support'\n",
    "\n",
    "evaluator = OntologyQualityEvaluator(LUCADAOntology())\n",
    "results = evaluator.evaluate()\n",
    "\n",
    "print('=== LUCADA Ontology Quality Evaluation ===')\n",
    "print(f'{'Competency Question':<65} {'Score':>6} {'Assessment'}')\n",
    "print('-' * 120)\n",
    "total = 0\n",
    "for r in results:\n",
    "    bar = '' * int(r['score'] * 10) + '' * (10 - int(r['score'] * 10))\n",
    "    print(f'{r[\"cq\"]:<65} {r[\"score\"]:>5.1f} {bar} {r[\"reason\"]}')\n",
    "    total += r['score']\n",
    "\n",
    "avg = total / len(results)\n",
    "print(f'\\nOverall Score: {avg:.2f}/1.0 ({avg*100:.0f}%)')\n",
    "print(f'\\nGaps requiring GoingMeta patterns:')\n",
    "for r in results:\n",
    "    if r['score'] < 0.5:\n",
    "        print(f'  - {r[\"cq\"]}: {r[\"reason\"]}')"
]))

# ============================================================
# PHASE 7: Missing Agents & Services
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 17. Extended Agent Testing\n",
    "\n",
    "Test all agents not covered in the original notebook:\n",
    "LabInterpretation, MedicationManagement, MonitoringCoordinator,\n",
    "DynamicOrchestrator, DigitalTwin, ClinicalTrialMatcher."
]))

new_cells.append(make_code_cell([
    "# Test LabInterpretation Agent\n",
    "from backend.src.agents.lab_interpretation_agent import LabInterpretationAgent\n",
    "\n",
    "lab_agent = LabInterpretationAgent()\n",
    "\n",
    "lab_data = {\n",
    "    'patient_id': 'PAT_NOTEBOOK_001',\n",
    "    'results': [\n",
    "        {'test': 'WBC', 'value': 3.2, 'unit': '10^9/L', 'reference_low': 4.0, 'reference_high': 11.0},\n",
    "        {'test': 'Hemoglobin', 'value': 10.5, 'unit': 'g/dL', 'reference_low': 12.0, 'reference_high': 17.5},\n",
    "        {'test': 'Creatinine', 'value': 1.8, 'unit': 'mg/dL', 'reference_low': 0.7, 'reference_high': 1.3},\n",
    "        {'test': 'ALT', 'value': 45, 'unit': 'U/L', 'reference_low': 7, 'reference_high': 56},\n",
    "        {'test': 'Platelets', 'value': 145, 'unit': '10^9/L', 'reference_low': 150, 'reference_high': 400},\n",
    "    ]\n",
    "}\n",
    "\n",
    "lab_result = await lab_agent.interpret(lab_data) if asyncio.iscoroutinefunction(getattr(lab_agent, 'interpret', None)) else lab_agent.interpret(lab_data)\n",
    "print('=== Lab Interpretation Results ===')\n",
    "pprint(lab_result)"
]))

new_cells.append(make_code_cell([
    "# Test MedicationManagement Agent\n",
    "from backend.src.agents.medication_management_agent import MedicationManagementAgent\n",
    "\n",
    "med_agent = MedicationManagementAgent()\n",
    "\n",
    "med_data = {\n",
    "    'patient_id': 'PAT_NOTEBOOK_001',\n",
    "    'current_medications': ['osimertinib', 'metformin', 'amlodipine'],\n",
    "    'proposed_treatment': 'carboplatin/pemetrexed',\n",
    "    'comorbidities': ['COPD', 'diabetes'],\n",
    "    'renal_function': {'creatinine': 1.8, 'gfr': 45}\n",
    "}\n",
    "\n",
    "med_result = await med_agent.assess(med_data) if asyncio.iscoroutinefunction(getattr(med_agent, 'assess', None)) else med_agent.assess(med_data)\n",
    "print('=== Medication Management Results ===')\n",
    "pprint(med_result)"
]))

new_cells.append(make_code_cell([
    "# Test MonitoringCoordinator Agent\n",
    "from backend.src.agents.monitoring_coordinator_agent import MonitoringCoordinatorAgent\n",
    "\n",
    "monitor_agent = MonitoringCoordinatorAgent()\n",
    "\n",
    "monitor_data = {\n",
    "    'patient_id': 'PAT_NOTEBOOK_001',\n",
    "    'treatment': 'Osimertinib (EGFR TKI)',\n",
    "    'stage': 'IIIA',\n",
    "    'comorbidities': ['COPD'],\n",
    "    'performance_status': 1\n",
    "}\n",
    "\n",
    "monitor_result = await monitor_agent.create_protocol(monitor_data) if asyncio.iscoroutinefunction(getattr(monitor_agent, 'create_protocol', None)) else monitor_agent.create_protocol(monitor_data)\n",
    "print('=== Monitoring Protocol ===')\n",
    "pprint(monitor_result)"
]))

new_cells.append(make_code_cell([
    "# Test DynamicOrchestrator\n",
    "from backend.src.agents.dynamic_orchestrator import DynamicWorkflowOrchestrator, WorkflowComplexity\n",
    "\n",
    "orchestrator = DynamicWorkflowOrchestrator()\n",
    "\n",
    "# Test complexity assessment\n",
    "test_cases_orch = [\n",
    "    {'age': 45, 'sex': 'M', 'tnm_stage': 'IA', 'histology_type': 'adenocarcinoma', 'performance_status': 0},\n",
    "    {'age': 68, 'sex': 'M', 'tnm_stage': 'IIIA', 'histology_type': 'adenocarcinoma', 'performance_status': 1,\n",
    "     'biomarker_profile': {'egfr': True}, 'comorbidities': ['COPD']},\n",
    "    {'age': 78, 'sex': 'F', 'tnm_stage': 'IV', 'histology_type': 'squamous_cell', 'performance_status': 3,\n",
    "     'biomarker_profile': {'pdl1_tps': 80, 'kras': True}, 'comorbidities': ['cardiovascular_disease', 'diabetes', 'COPD']},\n",
    "]\n",
    "\n",
    "print('=== Dynamic Orchestrator: Complexity Assessment ===')\n",
    "for i, case in enumerate(test_cases_orch, 1):\n",
    "    complexity = orchestrator.assess_complexity(case)\n",
    "    print(f'  Case {i} (Stage {case[\"tnm_stage\"]}, PS {case[\"performance_status\"]}): '\n",
    "          f'{complexity.complexity.value} -> {complexity.recommended_workflow}')"
]))

new_cells.append(make_code_cell([
    "# Test Digital Twin Engine\n",
    "from backend.src.digital_twin.twin_engine import DigitalTwinEngine\n",
    "\n",
    "try:\n",
    "    twin_engine = DigitalTwinEngine()\n",
    "    \n",
    "    twin_data = {\n",
    "        'patient_id': 'PAT_NOTEBOOK_001',\n",
    "        'age': 68, 'sex': 'M', 'tnm_stage': 'IIIA',\n",
    "        'histology_type': 'adenocarcinoma',\n",
    "        'performance_status': 1,\n",
    "        'biomarker_profile': {'egfr': 'Exon 19 deletion'},\n",
    "        'treatment': 'Osimertinib'\n",
    "    }\n",
    "    \n",
    "    twin_result = twin_engine.create_twin(twin_data)\n",
    "    print('=== Digital Twin Created ===')\n",
    "    pprint(twin_result)\n",
    "except Exception as e:\n",
    "    print(f'Digital Twin not available: {e}')"
]))

new_cells.append(make_code_cell([
    "# Test Clinical Trial Matcher\n",
    "from backend.src.analytics.clinical_trial_matcher import ClinicalTrialMatcher\n",
    "\n",
    "try:\n",
    "    trial_matcher = ClinicalTrialMatcher()\n",
    "    \n",
    "    trial_patient = {\n",
    "        'age': 68, 'sex': 'M', 'tnm_stage': 'IIIA',\n",
    "        'histology_type': 'adenocarcinoma',\n",
    "        'performance_status': 1,\n",
    "        'biomarker_profile': {'egfr': True, 'pdl1_tps': 50}\n",
    "    }\n",
    "    \n",
    "    matches = trial_matcher.find_matches(trial_patient)\n",
    "    print('=== Clinical Trial Matches ===')\n",
    "    for m in matches[:5]:\n",
    "        print(f'  Trial: {m.get(\"trial_id\", \"N/A\")} - {m.get(\"title\", \"N/A\")}')\n",
    "        print(f'    Score: {m.get(\"match_score\", 0):.2f}, Phase: {m.get(\"phase\", \"N/A\")}')\n",
    "except Exception as e:\n",
    "    print(f'Clinical Trial Matcher: {e}')"
]))

# ============================================================
# PHASE 8: Advanced Capabilities
# ============================================================
new_cells.append(make_md_cell([
    "---\n",
    "## 18. Advanced Capabilities\n",
    "\n",
    "- Ontology visualization (OWL to Mermaid, GoingMeta S39)\n",
    "- Graph algorithms on patient cohorts\n",
    "- Reflection agent pattern (GoingMeta S27)\n",
    "- NL generation from graph annotations (GoingMeta S7)"
]))

new_cells.append(make_md_cell([
    "### 18.1 Ontology to Mermaid Diagram\n",
    "\n",
    "Convert LUCADA ontology to Mermaid diagram for stakeholder communication.\n",
    "Adapted from GoingMeta Session 39."
]))

new_cells.append(make_code_cell([
    "# OWL to Mermaid: Visualize ontology as a diagram\n",
    "# Adapted from GoingMeta Session 39\n",
    "\n",
    "def ontology_to_mermaid(onto, max_classes=30) -> str:\n",
    "    \"\"\"Convert OWL ontology to Mermaid class diagram.\"\"\"\n",
    "    lines = ['classDiagram']\n",
    "    \n",
    "    classes_shown = set()\n",
    "    for cls in list(onto.classes())[:max_classes]:\n",
    "        name = cls.name\n",
    "        if not name or name == 'Thing':\n",
    "            continue\n",
    "        classes_shown.add(name)\n",
    "        \n",
    "        # Add class with key properties\n",
    "        lines.append(f'    class {name} {{')\n",
    "        # Add data properties relevant to this class\n",
    "        for prop in onto.data_properties():\n",
    "            if cls in prop.domain:\n",
    "                lines.append(f'        +{prop.name}')\n",
    "        lines.append('    }')\n",
    "    \n",
    "    # Add inheritance relationships\n",
    "    for cls in list(onto.classes())[:max_classes]:\n",
    "        if cls.name not in classes_shown:\n",
    "            continue\n",
    "        for parent in cls.is_a:\n",
    "            if hasattr(parent, 'name') and parent.name in classes_shown:\n",
    "                lines.append(f'    {parent.name} <|-- {cls.name}')\n",
    "    \n",
    "    # Add object property relationships\n",
    "    for prop in onto.object_properties():\n",
    "        for d in prop.domain:\n",
    "            for r in prop.range:\n",
    "                if hasattr(d, 'name') and hasattr(r, 'name'):\n",
    "                    if d.name in classes_shown and r.name in classes_shown:\n",
    "                        lines.append(f'    {d.name} --> {r.name} : {prop.name}')\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "from backend.src.ontology.lucada_ontology import LUCADAOntology\n",
    "onto = LUCADAOntology().create()\n",
    "mermaid_code = ontology_to_mermaid(onto, max_classes=25)\n",
    "print('=== LUCADA Ontology as Mermaid Diagram ===')\n",
    "print(mermaid_code)\n",
    "print(f'\\n(Paste into https://mermaid.live to render)')"
]))

new_cells.append(make_md_cell([
    "### 18.2 Graph Algorithms on Patient Cohorts"
]))

new_cells.append(make_code_cell([
    "# Run graph algorithms on patient data\n",
    "print('=== Graph Statistics ===')\n",
    "\n",
    "# Node/relationship counts\n",
    "stats = run_query(\"\"\"\n",
    "    MATCH (n)\n",
    "    WITH labels(n) as lbls, count(n) as cnt\n",
    "    UNWIND lbls as label\n",
    "    RETURN label, sum(cnt) as count\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "print('\\nNode counts by label:')\n",
    "for s in stats:\n",
    "    print(f'  {s[\"label\"]}: {s[\"count\"]}')\n",
    "\n",
    "# Relationship counts\n",
    "rel_stats = run_query(\"\"\"\n",
    "    MATCH ()-[r]->()\n",
    "    RETURN type(r) as rel_type, count(r) as count\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "print('\\nRelationship counts by type:')\n",
    "for s in rel_stats:\n",
    "    print(f'  {s[\"rel_type\"]}: {s[\"count\"]}')\n",
    "\n",
    "# Connected components\n",
    "components = run_query(\"\"\"\n",
    "    MATCH (p:Patient)\n",
    "    OPTIONAL MATCH path = (p)-[*1..3]-()\n",
    "    WITH p, count(DISTINCT nodes(path)) as neighborhood_size\n",
    "    RETURN p.patient_id as patient, neighborhood_size\n",
    "    ORDER BY neighborhood_size DESC\n",
    "\"\"\")\n",
    "print('\\nPatient neighborhood sizes:')\n",
    "for c in components:\n",
    "    print(f'  {c[\"patient\"]}: {c[\"neighborhood_size\"]} connected nodes')"
]))

new_cells.append(make_md_cell([
    "### 18.3 Experiment Summary Dashboard"
]))

new_cells.append(make_code_cell([
    "# Final comprehensive summary of all capabilities tested\n",
    "print('=' * 70)\n",
    "print('COMPREHENSIVE EXPERIMENT SUMMARY')\n",
    "print('=' * 70)\n",
    "\n",
    "capabilities = [\n",
    "    ('Ontology Construction', 'Section 2', 'PASS'),\n",
    "    ('Guideline Rules Engine', 'Section 3', 'PASS'),\n",
    "    ('6-Agent Pipeline', 'Section 5', 'PASS'),\n",
    "    ('Specialized Agents (Biomarker/NSCLC/Comorbidity)', 'Section 6', 'PASS'),\n",
    "    ('Multi-Patient Cohort', 'Section 7', 'PASS'),\n",
    "    ('Batch Processing', 'Section 8', 'PASS'),\n",
    "    ('Analytics (Survival/Uncertainty/Counterfactual)', 'Section 9', 'PASS'),\n",
    "    ('Conversational Assistant', 'Section 11', 'NEW'),\n",
    "    ('Neo4j CRUD + Text2Cypher', 'Section 12', 'NEW'),\n",
    "    ('Graph Visualization (pyvis)', 'Section 13', 'NEW'),\n",
    "    ('Provenance Tracking (W3C PROV-DM)', 'Section 14', 'NEW'),\n",
    "    ('Context Graph & Grounded Citations', 'Section 15', 'NEW'),\n",
    "    ('SHACL-style Validation', 'Section 16.1', 'NEW'),\n",
    "    ('Ontology-Driven RAG', 'Section 16.2', 'NEW'),\n",
    "    ('Semantic Patient Similarity', 'Section 16.3', 'NEW'),\n",
    "    ('Ontology Quality Evaluation', 'Section 16.4', 'NEW'),\n",
    "    ('Extended Agents (Lab/Med/Monitor/Orchestrator)', 'Section 17', 'NEW'),\n",
    "    ('Digital Twin Engine', 'Section 17', 'NEW'),\n",
    "    ('Clinical Trial Matching', 'Section 17', 'NEW'),\n",
    "    ('Ontology to Mermaid', 'Section 18.1', 'NEW'),\n",
    "    ('Graph Algorithms', 'Section 18.2', 'NEW'),\n",
    "]\n",
    "\n",
    "print(f'\\n{\"Capability\":<55} {\"Section\":<15} {\"Status\"}')\n",
    "print('-' * 80)\n",
    "for name, section, status in capabilities:\n",
    "    icon = '' if status == 'PASS' else '' if status == 'NEW' else ''\n",
    "    print(f'  {icon} {name:<53} {section:<15} {status}')\n",
    "\n",
    "existing = sum(1 for _, _, s in capabilities if s == 'PASS')\n",
    "new = sum(1 for _, _, s in capabilities if s == 'NEW')\n",
    "print(f'\\nTotal: {len(capabilities)} capabilities ({existing} existing + {new} new)')\n",
    "print(f'\\nGoingMeta Patterns Integrated: S3 (SHACL), S16 (Similarity), S24 (RAG), S39 (Mermaid), S42 (Quality)')"
]))

# ========================================
# Write the cells to the notebook
# ========================================
with open('H:/akash/git/CoherencePLM/Version22/experiment.ipynb', 'r', encoding='utf-8') as f:
    nb = json.load(f)

nb['cells'].extend(new_cells)

with open('H:/akash/git/CoherencePLM/Version22/experiment.ipynb', 'w', encoding='utf-8') as f:
    json.dump(nb, f, indent=1, ensure_ascii=False)

print(f"Added {len(new_cells)} new cells to experiment.ipynb")
print(f"Total cells now: {len(nb['cells'])}")
